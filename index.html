<!doctype html>
<meta charset="utf-8">
<script src="https://distill.pub/template.v1.js"></script>

<style>
  .figure .caption {
    font-size: 16px;
    line-height: 18px;
    margin: 0;
  }
  .figure .caption-centered {
    font-size: 16px;
    line-height: 18px;
    margin: 0;
    text-align: center;
  }
  .figure .figure-content {
    width: 100%;
  }
  .columns {
    display: flex;
    flex-direction: row;
    justify-content: space-between;
  }
  .columns .column-2 {
    width: 50%;
    padding: 10px;
  }
  .columns .column-4 {
    width: 25%;
    padding: 10px;
  }
</style>

<script type="text/front-matter">
  title: "Adversarial Agents in MARL"
  description: "Description of the post"
  authors:
  - Jarek Liesen
  affiliations:
  - BCCN Berlin: https://www.bccn-berlin.de
</script>

<dt-article>
  <h1>Adversarial Agents in multi-agent reinforcement learning</h1>
  <!-- <h2>A description of the article</h2> -->
  <dt-byline></dt-byline>
  <!-- Introduction -->
  <p>
    Adversarial attacks are a well-known phenomenon in machine learning that poses a threat to the security of many practical applications.
    To attack a neural network, one adds a small pertubation to its input, which leads to a misclassification.
    They have been studied extensively in the context of computer vision, and more recently in reinforcement learning (RL).
  </p>
  <div class="l-page side">
    <div class="columns">
      <div class="column-2 figure">
        <img class="figure-content" src="static/hopper.gif">
        <p class="caption">Hopper agent trained with PPO under normal circumstances<br><b>Reward</b>: 3104</p>
      </div>
      <div class="column-2 figure">
        <img class="figure-content" src="static/hopper_pa.gif">
        <p class="caption">The same Hopper agent under adversarial attack from <dt-cite key="sun2021who"></dt-cite><br><b>Reward</b>: 171</p>
      </div>
    </div>
  </div>
  <p>
    In an RL setting, perturbed observations can fool an agent with a neural network policy into taking actions that lead to poor performance
    <dt-cite key="huang2017adversarial"></dt-cite><dt-cite key="chen2019adversarial"></dt-cite>.
    For example, look at the hopper on the right, where the observations have been unnoticeably perturbed, leading to a detrimental drop in the cumulative reward.
    There is a multitude of methods to attack RL agents, including classical methods such as FGSM <dt-cite key="goodfellow2014explaining"></dt-cite>,
    and methods that exploit RL specific properties <dt-cite key="kos2017delving"></dt-cite><dt-cite key="sun2021who"></dt-cite>.
    The majority of these methods assume that the attacker is able to freely modify the agent's observations, which might not be possible in many cases.
    However, an interesting opportunity arises in multi-agent RL (MARL) settings, where agents observe each other:
  </p>
  <p><i>An attacker might take the form of an agent in MARL, whose actions directly influence the victim's observations.</i></p>
  <p>
    This is the key idea in Gleave et al's paper "Adversarial Policies: Attacking Deep Reinforcement Learning" <dt-cite key="gleave2020adversarial"></dt-cite>,
    who train adversarial agents that attack victims in several two-player games.
    In this post, we will take a closer look at those adversarial agents, analyze how they work, and discuss ways to defend against them.
  </p>
  <h2>Setting things up</h2>
  <p>
    We will train adversarial agents on four different mujoco environments, each of which implements a two-player, zero-sum game.
    Originally, these environments have been introduced in Bansal et al's paper "Emergent	Complexity via Multi-Agent Competition" <dt-cite key="bansal2018emergent"></dt-cite>,
    who trained a several agents competing in each environment.
    For our experiments, we will focus on the following four:
  </p>
  <div class="l-page-outset columns">
    <!-- All videos are from the zoo agents with tag 1 -->
    <div class="column-4 figure">
      <video class="figure-content" autoplay loop controls>
        <source src="static/you-shall-not-pass-zoo-concat.mp4" type="video/mp4">
      </video>
      <p class="caption-centered">
        <b>You Shall Not Pass</b> (humans) <br>
        Asymmetric, MLP policy
      </p>
    </div>
    <div class="column-4 figure">
      <video class="figure-content" autoplay loop controls>
        <source src="static/kick-and-defend-zoo-concat.mp4" type="video/mp4">
      </video>
      <p class="caption-centered">
        <b>Kick and Defend</b> (humans)<br>
        Asymmetric, LSTM policy
      </p>
    </div>
    <div class="column-4 figure">
      <video class="figure-content" autoplay loop controls>
        <source src="static/sumo-humans-zoo-concat.mp4" type="video/mp4">
      </video>
      <p class="caption-centered">
        <b>Sumo</b> (humans)<br>
        Symmetric, LSTM policy
      </p>
    </div>
    <div class="column-4 figure">
      <video class="figure-content" autoplay loop controls>
        <source src="static/sumo-ants-zoo-concat.mp4" type="video/mp4">
      </video>
      <p class="caption-centered">
        <b>Sumo</b> (ants)<br>
        Symmetric, LSTM policy
      </p>
    </div>
  </div>
  <p>
    The score you see above has the form <i>#wins - #losses - #draws</i>, from the perspective of one of the agents, which we will call the attacker.
    Right now, the attacker acts according to its pretrained policy, but later we will train an adversarial policy to act in its place.
    <ul>
      <li>In <b>You Shall Not Pass</b> the attacker is the red agent, who wins if the blue agent falls to the ground before it reaches the red line, and loses otherwise.</li>
      <li>
        In <b>Kick and Defend</b> the attacker is the blue agent, who wins if the red agent fails to score, but the ball remains within the vicinity of the goal.
        The blue agent loses if a goal is scored, otherwise it is a draw.
      </li>
      <li>In <b>Sumo</b> the attacker is the blue agent. An agent wins if it manages to stand while its opponent falls to the ground our outside the arena.</li>
    </ul>
  </p>

</dt-article>

<dt-appendix>
</dt-appendix>

<script type="text/bibliography">
  @article{gleave2020adversarial,
    title={Adversarial Policies: Attacking Deep Reinforcement Learning},
    author={Gleave, Adam and Dennis, Michael and Wild, Cody and Kant, Neel and Levine, Sergey and Russel, Stuart},
    journal={ICLR 2020},
    year={2020},
    url={https://openreview.net/forum?id=HJgEMpVFwB}
  }
  @article{chen2019adversarial,
    title={Adversarial attack and defense in reinforcement learning-from AI security view},
    author={Chen, Tong and Liu, Jiqiang and Xiang, Yingxiao and Niu, Wenjia and Tong, Endong and Han, Zhen},
    journal={Cybersecurity},
    year={2019},
    url={https://doi.org/10.1186/s42400-019-0027-x}
  }
  @article{goodfellow2014explaining,
    title={Explaining and harnessing adversarial examples},
    author={Goodfellow, Ian and Shlens, Jonathon and Szegedy, Christian},
    journal={CoRR},
    year={2014},
    url={https://arxiv.org/abs/1412.6572}
  }
  @article{kos2017delving,
    title={Delving into adversarial attacks on deep policies},
    author={Kos, Jernej and Song, Dawn},
    journal={ICLR 2017 workshop submission},
    year={2017},
    url={https://openreview.net/forum?id=BJcib5mFe}
  }
  @article{sun2021who,
    title={Who is the strongest enemy? Towards Optimal and Efficient Attacks in Deep RL},
    author={Sun, Yanchao and Zheng, Ruijie and Liang, Yongyuan and Huang, Furong},
    journal={ICLR 2022 Poster},
    year={2022},
    url={https://openreview.net/forum?id=JM2kFbJvvI}
  }
  @article{huang2017adversarial,
    title={Adversarial Attacks on Neural Network Policies},
    author={Huang, Sandy and Papernot, Nicolas and Goodfellow, Ian and Duan, Yan and Abbeel, Pieter},
    journal={ICLR 2017 workshop submission},
    year={2017},
    url={https://openreview.net/forum?id=ryvlRyBKl}
  }
  @article{bansal2018emergent,
    title={Emergent Complexity via Multi-Agent Competition},
    author={Bansal, Trapit and Pachocki, Jakub and Sutskever, Ilya and Mordatch, Igor},
    journal={ICLR 2018 conference submission},
    year={2018},
    url={https://openreview.net/forum?id=Sy0GnUxCb}
  }
</script>
